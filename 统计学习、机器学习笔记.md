1. **线性回归**
    - 线性模型有更好的解释，其系数表示相关性的强
    - [**矩阵求导**](<https://zhuanlan.zhihu.com/p/24709748>)

2. **逻辑回归**
    - **2.1 LDA**
        + LDA通常作为数据预处理阶段的降维技术，其目标是将数据投影到低维空间来避免维度灾难（curse of dimensionality）引起的过拟合，同时还保留着良好的可分性.
        + 结合**PCA**一起看，加深理解。
        + PCA（主成分分析）和LDA（线性判别分析）有很多的相似点，其本质是要将初始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说**PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法**。
        + 主成分分析（PCA）与LDA有着非常近似的意思，LDA的输入数据是带标签的，是一种**监督学习**，而PCA的输入数据是不带标签的，所以PCA是一种**无监督学习**
        + [LDA原理总结](<https://www.cnblogs.com/pinard/p/6244265.html>)
        + [LDA线性判别分析原理篇](<https://zhuanlan.zhihu.com/p/27899927>)
        + [案例](<https://flashgene.com/archives/26694.html>)
    - [【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)
    - **2.2 多项逻辑回归**
        + [多项逻辑回归](https://blog.csdn.net/pxhdky/article/details/83050712)
    - **2.3 最大熵模型**
        + [最大熵原理和最大熵模型](https://www.hrwhisper.me/machine-learning-maximum-entropy-model/)
        + [最大熵模型原理小结](https://www.cnblogs.com/pinard/p/6093948.html)
        + [最大熵原理学习笔记](https://blog.csdn.net/itplus/article/details/26550597)
        + [如何理解最大熵模型里面的特征](https://www.zhihu.com/question/24094554/answer/108271031)
        + [“熵”不起：从熵、最大熵原理到最大熵模型（三）](https://spaces.ac.cn/archives/3567)
        + [**支持向量机通俗导论（理解SVM的三层境界）**](https://blog.csdn.net/v_july_v/article/details/7624837)
        + [熵、互信息、相对熵](https://zhuanlan.zhihu.com/p/36192699)
        + 互信息是二个随机事件的相关性的量化度量。相对熵也用来衡量相关性，但是和变量的互信息不同，它用来衡量**两个取值为正数的函数**的相似性，而且还是**不对称**的。
        + [图形说明](https://blog.csdn.net/qq_22238533/article/details/77774223)

3. **多分类学习(multi-class learning)**
    - [周志华版本](http://next.uuzdaisuki.com/2018/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7-%E2%80%94%E2%80%94%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0/)
    
4. **多标签学习（multi-label learning）**
    + [论文](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6471714)
    + [常用策略](https://www.zhihu.com/question/35486862)
    + [多标记学习初探](https://www.jianshu.com/p/218f2a3930f2)
    + [softmax如何作用与多标签分类](https://www.zhihu.com/question/36031920)
    
5. **分类器的预测置信度**
    + [基础知识](https://zhuanlan.zhihu.com/p/37847495)
    + [置信区间](https://www.zhihu.com/question/26419030)
    
6. **MLE,MAP,bayesian estimate**
    + [比较好的比喻](https://zhuanlan.zhihu.com/p/37215276)
    + [不错的总结](http://noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/)
    + [英文版本](https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9)
7. **模型选择**
    + [泛化误差](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/9_model_selection.html)
    + [集成学习降低方差和偏差](https://www.zhihu.com/question/26760839)
    + [集成学习](https://www.cnblogs.com/stream886/p/10506793.html)
    + [生成模型与判别模型](https://blog.csdn.net/zouxy09/article/details/8195017)
    + [精度和召回率](https://www.zhihu.com/question/19645541)
    
8. **感知机**
    + [感知机学习算法的对偶形式](https://www.zhihu.com/question/26526858)
    
9. **参数模型和非参数模型**
    + 最重要的区别是：**参数模型**有固定数量的参数；而**非参数模型**随着数据量的增加，模型参数的数量也增加。参数模型的优点是使用速度快，缺点是对数据分布的性质做了更强的假设。非参数模型更加灵活，但对于大型数据集来说，通常在计算上比较棘手。**参数模型有：线性回归，逻辑回归，感知机，朴素贝叶斯，LDA，线性支持向量机等。非参数模型有：KNN，决策树，高斯核支持向量机，神经网络等。**

10. **图卷积**
    - [快速了解GCN](https://blog.csdn.net/u011537121/article/details/81542991)
    - [理解图卷积](https://www.zhihu.com/question/54504471)

11. **最大似然**
    - [从最大似然到EM算法：一致的理解方式](https://kexue.fm/archives/5239)
    - [梯度下降和EM算法：系出同源，一脉相承](https://kexue.fm/archives/4277)
    - [理解交叉熵](https://juejin.im/post/5b28bd26f265da59bb0cc8f6)
    
12. **采样**
    - [LDA-math-MCMC 和 Gibbs Sampling](https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling/)
    
13. **EM-最大期望算法**
    - [详解](http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/#%E4%BA%8C%E3%80%81Jensen%E4%B8%8D%E7%AD%89%E5%BC%8F)
    - [GMM与EM算法的Python实现](http://sofasofa.io/tutorials/gmm_em/)

14. **牛顿法和拟牛顿法**
    + 牛顿法虽然收敛速度快（因为不仅考虑了梯度，而且考虑了梯度的加速方向）
    + [牛顿法与拟牛顿法学习笔记](https://blog.csdn.net/itplus/article/details/21896453)
    + [梯度下降、牛顿法、拟牛顿法](https://blog.csdn.net/a819825294/article/details/52172463)
    + [梯度下降法、牛顿法和拟牛顿法](https://zhuanlan.zhihu.com/p/37524275)
    + [牛顿法面试](https://zhuanlan.zhihu.com/p/31229539)
    + [机器学习中牛顿法凸优化的通俗解释](https://juejin.im/post/5b32e6ee6fb9a00e4e47c1c7)
    
15. **启发式算法**
    + [什么是启发式（heuristic）?](https://www.cnblogs.com/p2pstream/archive/2009/04/09/1432270.html)
    + [启发式算法（Heuristic Algorithm）](https://blog.csdn.net/u010159842/article/details/75530645)

16. **决策树**
    + [决策树算法原理(上)](https://www.cnblogs.com/pinard/p/6050306.html)
    + [决策树算法原理(下)](https://www.cnblogs.com/pinard/p/6053344.html)

17. **SVM**
    + [拉格朗日乘子法、KKT条件、拉格朗日对偶性](https://blog.csdn.net/sinat_17496535/article/details/52103852)
    + [拉格朗日乘子法 - KKT条件 - 对偶问题](https://www.cnblogs.com/massquantity/p/10807311.html)
    + [约束优化方法：拉格朗日乘子法与KKT条件](https://deepindeed.cn/2019/03/23/lagrangian/#%E5%BC%95%E8%A8%8018)

18. **贝叶斯网络**
    + [贝叶斯网络的理解](https://www.cnblogs.com/mantch/p/11179933.html)
