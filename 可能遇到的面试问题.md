- [machinelearning](https://github.com/ljpzzz/machinelearning) 刘建平Pinard的博客，有问题可以第一时间从这里找解答，讲的比较的细致和全面
- **如何防止模型过拟合的问题**
    + [机器学习中用来防止过拟合的方法有哪些？- 知乎](https://www.zhihu.com/question/59201590)
    + [机器学习中防止过拟合的处理方法](https://blog.csdn.net/heyongluoyao8/article/details/49429629)
    + [归一化 （Normalization）、标准化 （Standardization）和中心化/零均值化 （Zero-centered）,BN,Batch,批归一化,从归一化到批归一化](https://blog.csdn.net/qq_35290785/article/details/89322289)
    + [正则化](https://www.cnblogs.com/maybe2030/p/9231231.html#_label5)
- **常见跳出局部最小值策略**
    + [西瓜书给出的解决方法](https://blog.csdn.net/Touch_Dream/article/details/70142482)
- **特征选择**
    + [特征选择常用算法综述](https://www.cnblogs.com/heaad/archive/2011/01/02/1924088.html)
    + [机器学习中，有哪些特征选择的工程方法](https://www.zhihu.com/question/28641663/answer/41653367)
    + [过滤式特征选择 - 1](https://www.cnblogs.com/wanglei5205/p/8973680.html)
    + [过滤式特征选择 - 2](https://blog.csdn.net/weixin_43378396/article/details/90649064)
- **随机森林，GBDT，XGBoost**
    + [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)
    + [XGBoost算法原理小结](https://www.cnblogs.com/pinard/p/10979808.html)
    + [GBDT算法原理以及实例理解](https://blog.csdn.net/zpalyq110/article/details/79527653)
    + [随机森林，GBDT，XGBoost的对比](https://blog.csdn.net/yingfengfeixiang/article/details/80210145)
    + [Boosting模型：XGBoost原理剖析](https://www.csuldw.com/2019/07/20/2019-07-20-xgboost-theory/)
    + [LightGBM 算法原理](https://my.oschina.net/sunmin/blog/3041224)
    + [Boosting模型：lightGBM 算法原理](https://www.csuldw.com/2019/07/24/2019-07-24-an-introduction-tolightGBM-explained/?utm_source=tuicool&utm_medium=referral)
    + [机器学习 之LightGBM算法](https://www.cnblogs.com/hugechuanqi/p/10584602.html)
- [**BAT机器学习面试1000题系列**](https://blog.csdn.net/sinat_35512245/article/details/78796328)
- Dropout原理和实现
    + [Dropout浅层理解与实现](https://blog.csdn.net/hjimce/article/details/50413257)
    + [Dropout原理与实现](https://www.cnblogs.com/zingp/p/11631913.html)
    + [深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)
    + [深度学习网络大杀器之Dropout——深入解析Dropout](https://www.cnblogs.com/bonelee/p/8127451.html)
- [神经网络优化方法](https://zhuanlan.zhihu.com/p/29779000)
- [深度学习中Dropout原理解析](https://blog.csdn.net/program_developer/article/details/80737724)
- [二元分类为什么不能用MSE做为损失函数？](http://sofasofa.io/forum_main_post.php?postid=1001792)
- 排序算法
    + [面试中的 10 大排序算法总结](http://www.codeceo.com/article/10-sort-algorithm-interview.html#0-tsina-1-10490-397232819ff9a47a7b7e80a40613cfe1)
    + [面试常用排序算法总结](http://huyan.couplecoders.tech/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/java%E9%9D%A2%E8%AF%95/2019/01/13/%E9%9D%A2%E8%AF%95%E5%B8%B8%E7%94%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/)
- [One-Hot Encoding 及其使用原因](https://blog.csdn.net/taotiezhengfeng/article/details/73692239)
- [梯度下降优化算法综述](https://blog.csdn.net/google19890102/article/details/69942970)
- [详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526)
- [理解 LSTM 网络 （Understanding LSTM Networks by colah）](https://blog.csdn.net/Jerr__y/article/details/58598296)
- [10亿int型数，统计只出现一次的数](https://blog.csdn.net/u010983881/article/details/75097358)
- [完美二叉树, 完全二叉树和完满二叉树](https://www.cnblogs.com/idorax/p/6441043.html)
- [面试/笔试第二弹 —— 操作系统面试问题集锦](https://blog.csdn.net/justloveyou_/article/details/78304294)
- [看图理解单链表的反转](https://blog.csdn.net/feliciafay/article/details/6841115) **可以认为单链表是一颗永远只有左(右)子树的树，因此可以考虑用递归来解决。**这句话真的是理解链表的递归解法一览无余。
- [全面分析再动手的习惯：链表的反转问题（递归和非递归方式）](https://www.cnblogs.com/kubixuesheng/p/4394509.html) 这个链接里的递归写法才是正确的
- [最全MySQL面试60题和答案](https://github.com/caokegege/Interview/blob/master/db/%E6%9C%80%E5%85%A8MySQL%E9%9D%A2%E8%AF%9560%E9%A2%98%E5%92%8C%E7%AD%94%E6%A1%88.md)
- [面试/笔试第三弹 —— 数据库面试问题集锦](https://blog.csdn.net/justloveyou_/article/details/78308460)
- [Python 2 和 Python 3 有哪些主要区别？](https://www.zhihu.com/question/19698598)
- [Python---copy()、deepcopy()与赋值的区别](https://blog.csdn.net/u011630575/article/details/78604226)
- 偏差和方差
    + 西瓜书->泛化误差可分解为偏差、方差和噪声之和；偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。
- [百度笔试题：malloc/free与new/delete的区别](https://blog.csdn.net/Hackbuteer1/article/details/6789164)
- [为什么MySQL数据库索引选择使用B+树？](https://blog.csdn.net/xlgen157387/article/details/79450295)
- [Hierarchical Softmax详解](http://flyrie.top/2018/10/31/Word2vec_Hierarchical_Softmax/)
- [C/C++内存管理详解](https://chenqx.github.io/2014/09/25/Cpp-Memory-Management/)
- [高斯混合模型（GMM）及其EM算法的理解](https://blog.csdn.net/jinping_shi/article/details/59613054)
- [GMM与EM算法的Python实现](http://sofasofa.io/tutorials/gmm_em/)
- [软聚类，硬聚类？](http://sofasofa.io/forum_main_post.php?postid=1000539)
- [机器学习中的相似性度量](https://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html)
- [EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)
- [EM-最大期望算法](http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/)
- 图的存储和计算
    + [分布式图计算中的点切分和边切分](https://blog.csdn.net/qq_23377639/article/details/79550509)
    + [Spark学习之路 （二十八）分布式图计算系统](https://www.cnblogs.com/qingyunzong/p/9047754.html#_label2)
- hadoop详解和面试题汇总
    + [Hadoop详解(三)——MapReduce原理和执行过程，远程Debug，Writable序列化接口，MapReduce程序编写](https://blog.csdn.net/u013087513/article/details/77771600)
    + [MapReduce原理与设计思想](https://www.cnblogs.com/wuyudong/p/mapreduce-principle.html)
    + [Hadoop 学习系列（四）之 MapReduce 原理讲解](https://juejin.im/post/5bb59f87f265da0aeb7118f2)
    + [MapReduce面试题收集](https://blog.csdn.net/WYpersist/article/details/80102778)
- **深入理解AUC和ROC**
    + [ROC AUC有个非常直白的概率意义](http://sofasofa.io/forum_main_post.php?postid=1001008)
        * 随机挑选一个标签为0的样本A，再随机挑选一个标签为1的样本B。你预测样本B为1的概率大于样本A为1的概率的概率就是你的ROC AUC。比较拗口，多念几遍，就通顺了，嘿嘿嘿。
        * 我再多说几句，**ROC AUC的数值与每个预测概率的数值大小无关**，**在乎的是每个预测概率的排序。假设我们按照概率从大到小排。如果根据你的预测结果，所有标签为1的样本都排在了标签为0的样本前面，那么你的ROC AUC就是1。**
        * ROC AUC = 0.8的意思是说，随机挑选个标签为1的样本，它被排在随机的0样本的前面的概率是0.8。显然ROC AUC是0.5的话，就说明这个模型和随便猜没什么两样。
    + [理解AUC的意义](http://meroa.com/auc-awk-src) 
        * AUC的几何意义 AUC(Area Under Curves)指的是ROC曲线下的面积，该指标能较好的概括不平衡样本分类器的性能而成为很多数据挖掘竞赛的判定标准。由于仅有有限个样本，无论训练样本还是测试样本，因此无法获得最精确的ROC曲线，从而无法精确计算AUC。在实际计算中，使用类似微积分的方法，用梯形面积的和去近似。
        * AUC的**物理意义** 假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，**任取一对（正、负）样本，正样本的score大于负样本的score的概率**。从而我们能够理解对于AUC而言，并**不关心具体预测的结果是标签或者概率，也不需要卡什么阈值，只要在预测结果之间有排序即可**。
    + [理解 ROC 和 AUC](https://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/11/20/understanding-ROC-and-AUC)
        * 这里对公式的定义比较好理解一点。被用来评价一个二值分类器（binary classifier）的优劣。相比准确率、召回率、F-score这样的评价指标，**ROC曲线有这样一个很好的特性：当测试集中正负样本的分布变化的时候，ROC曲线能够保持不变。**在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。
        * AUC的物理含义：首先AUC值是一个概率值，**当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类**。另外，AUC与Gini分数有联系，Gini + 1 = 2*AUC。
+ [如何理解机器学习和统计中的AUC？](https://www.zhihu.com/question/39840928)
+ 参数模型和非参数模型
    * [非参模型是什么意思？有哪些模型算是非参的？](http://sofasofa.io/forum_main_post.php?postid=1001609)
    * [机器学习中参数模型和非参数模型理解](https://blog.csdn.net/FrankieHello/article/details/94022594)
    * [机器学习参数模型与非参数模型](https://www.deeplearn.me/1854.html)
    * [能不能用简明的语言解释什么是非参数（nonparametric）模型？](https://www.zhihu.com/question/22855599)
+ L1,L2正则化详细解释
    * 可以从解空间形状(等效成拉格朗日乘子法得解空间)，函数叠加，贝叶斯先验等
    * [深入理解L1、L2正则化](https://www.cnblogs.com/zingp/p/10375691.html)
    * [Laplace（拉普拉斯）先验与L1正则化](https://www.cnblogs.com/heguanyou/p/7688344.html)

+ [PCA 通过 SVD 分解替代协方差矩阵的特征值分解](https://blog.csdn.net/lw_power/article/details/89305054)
+ [那些年流行的面试智力题分享](https://www.cnblogs.com/XJT2018/p/11539661.html)
+ 图卷积的知识点
    + [GCN总结](https://www.cnblogs.com/nxf-rabbit75/p/11306198.html)
    + [[论文笔记]：GraphSAGE：Inductive Representation Learning on Large Graphs 论文详解 NIPS 2017](https://blog.csdn.net/yyl424525/article/details/100532849)
    + [GCMC - Graph Convolutional Matrix Completion 图卷积矩阵补全 KDD 2018](https://blog.csdn.net/yyl424525/article/details/102747805)
+ [布隆过滤器（Bloom Filter）原理以及应用](https://www.cnblogs.com/wuer888/p/11236164.html)
+ **推荐系统**
    * [推荐系统技术演进趋势：从召回到排序再到重排](https://zhuanlan.zhihu.com/p/100019681)
    * [推荐系统召回四模型之：全能的FM模型](https://zhuanlan.zhihu.com/p/58160982)
    * [推荐系统中召回策略](https://www.cnblogs.com/graybird/p/11393511.html)
    * [深入浅出ML之Factorization家族](http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/)
+ [聚集索引和非聚集索引的区别](https://blog.csdn.net/riemann_/article/details/90324846)
+ [排列组合的一些公式及推导(非常详细易懂)](https://www.cnblogs.com/1024th/p/10623541.html)
+ 大规模系统的问题
    * [大规模节点对向量相似度计算](https://cloud.google.com/solutions/machine-learning/building-real-time-embeddings-similarity-matching-system?hl=zh-cn)
+ 1*1卷积核的作用
    * [1*1卷积核的理解和作用](https://www.cnblogs.com/lqc-nogi/p/9740906.html)
    * [1*1卷积核作用](https://www.cnblogs.com/ywheunji/p/11005931.html)
+ [随机森林(random forest)和支持向量机(SVM)各有什么优劣](http://sofasofa.io/forum_main_post.php?postid=1000940)
+ [SVM，决策树，随机森林知识点整理](https://blog.csdn.net/weixin_42864175/article/details/88755913)
+ [海量数据处理 - 10亿个数中找出最大的10000个数（top K问题）](https://blog.csdn.net/zyq522376829/article/details/47686867)
+ [海量数据查找中位数](https://blog.csdn.net/qq_36770641/article/details/81395979)
+ [K-D TREE算法原理及实现](https://www.cnblogs.com/flyinggod/p/8727584.html)
+ [KNN（三）--KD树详解及KD树最近邻算法](https://blog.csdn.net/app_12062011/article/details/51986805)
+ [kNN算法：K最近邻(kNN，k-NearestNeighbor)分类算法](https://www.cnblogs.com/jyroy/p/9427977.html)
+ [KNN（最近邻算法）](https://www.cnblogs.com/xiaokangzi/p/4511871.html)
+ [高斯混合模型里的隐变量是什么变量？](http://sofasofa.io/forum_main_post.php?postid=1004524)
+ [**最大似然估计、最大后验估计、贝叶斯估计的对比**](https://www.cnblogs.com/jiangxinyang/p/9378535.html)
+ [三大抽样分布：卡方分布，t分布和F分布的简单理解](https://blog.csdn.net/anshuai_aw1/article/details/82735201)
+ [STL sort 函数实现详解](https://www.cnblogs.com/fengcc/p/5256337.html)
+ [为什么快速排序在数组的情况下比归并排序快](https://blog.csdn.net/love_fdu_llp/article/details/52288171?utm_source=blogkpcl8)
    * 快速排序效率的主要来源之一是引用的**局域性原理**，在这里计算机硬件被优化，因此访问彼此相邻的内存位置比访问分散在内存中的内存位置要快。快速排序中的分区步骤通常具有良好的局部性，因为它访问前后相邻的数组元素。因此，快速排序的性能往往比堆排序等其他排序算法要好得多，尽管它的比较和交换次数大致相同，因为在堆排序的情况下，访问更加分散。
    * 另外，快速排序通常比其他排序算法要快得多，因为它是**原址排序**的，不需要创建任何辅助数组来保存临时值。与归并排序相比，这是一个巨大的优势，因为分配和释放辅助数组所需的时间是显而易见的。就地操作还可以改进快速排序的位置。
    * 当**使用链表**时，这两个优点都不一定适用。因为链表单元常常分散在内存中，所以访问相邻的链表单元没有局部性的好处。因此，快速排序的一个巨大的性能优势被耗尽了。类似地，原址排序的好处不再适用，因为归并排序的链表算法不需要任何额外的辅助存储空间。
    * 也就是说，快速排序在链表上仍然非常快。归并排序往往更快，因为它更平均地将列表分成两半，并且在每次迭代中进行归并所做的工作比执行分区步骤要少。
+ [期望风险、经验风险与结构风险](https://alisure.github.io/2018/04/14/ML/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%93%E6%B7%B7%E6%A6%82%E5%BF%B5%E4%B9%8B%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E4%B8%8E%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9/)
+ [机器学习-->期望风险、经验风险与结构风险之间的关系](https://blog.csdn.net/liyajuan521/article/details/44565269)
+ [损失函数及经验风险和结构风险](https://www.cnblogs.com/always-fight/p/9483253.html)
+ [最大似然损失和交叉熵损失函数的联系](https://blog.csdn.net/diligent_321/article/details/53115369)
+ [哈？你还认为似然函数跟交叉熵是一个意思呀？](https://zhuanlan.zhihu.com/p/27719875)
+ [似然函数与最大似然估计、交叉熵概念与机器学习中的交叉熵函数](https://blog.csdn.net/zgcr654321/article/details/85204049)
+ [几种常见的损失函数](https://www.cnblogs.com/lliuye/p/9549881.html)
+ [教你理解机器学习中的各种范数](https://www.jianshu.com/p/b10abbf73b08)
    - 有监督的机器学习本质是“minimizeyour error while regularizing your parameters”——就是在最小化误差的同时规则化模型的参数。
    - 那为什么L1范数可以使参数稀疏呢？一种解释是“它是L0范数的最优凸近似”
    - 为什么使用L1范数而不是L0范数？因为求解L0范数是一个NP问题。而且L1范数是L0范数的最优凸近似，它比L0范数要容易优化求解。
+ [三大抽样分布：卡方分布，t分布和F分布的简单理解](https://blog.csdn.net/anshuai_aw1/article/details/82735201)
+ **混合高斯的隐变量是多项式分布**
+ [伯努利分布、二项分布、多项分布、Beta分布、Dirichlet分布](https://blog.csdn.net/Michael_R_Chang/article/details/39188321)
+ [最大似然估计、最大后验估计、贝叶斯估计的对比](https://www.cnblogs.com/jiangxinyang/p/9378535.html)
    + 极大似然估计的核心思想是：**认为当前发生的事件是概率最大的事件**。因此就可以给定的数据集，使得该数据集发生的概率最大来求得模型中的参数。似然函数如下：
    + 和最大似然估计不同的是，最大后验估计中引入了先验概率（先验分布属于贝叶斯学派引入的，像**L1，L2正则化就是对参数引入了拉普拉斯先验分布和高斯先验分布**），而且最大后验估计要求的是
+ [京东面经汇总](https://www.cnblogs.com/wupeixuan/p/8908524.html)
+ [距离计算方法总结](https://www.cnblogs.com/liujinhong/p/6001997.html)
+ [最优化：Lasso回归算法及其两种解法](https://blog.csdn.net/u012559269/article/details/80564606)
+ [图像处理--归一化切割--(normalized cut)--Python实现](https://blog.csdn.net/qq_38476684/article/details/80553850)
+ [类别型特征的处理方法与平均数编码](http://www.bewindoweb.com/217.html)
+ [平均数编码：针对某个分类特征类别基数特别大的编码方式](https://www.cnblogs.com/wzdLY/p/9639519.html)
+ [批量学习和在线学习的区别](https://www.cnblogs.com/Wanggcong/p/4699932.html)
+ [（批量学习、在线学习、基于实例的学习、基于模型的学习、核外学习）](https://blog.csdn.net/nishizhumashide/article/details/83019482)
+ [监督学习、非监督学习、批量学习、在线学习、基于实例学习、基于模型学习](https://blog.csdn.net/pxhdky/article/details/85063160)
+ [池化层理解](https://blog.csdn.net/weixin_38145317/article/details/89310404)
+ [神经网络中权重共享的是](https://www.nowcoder.com/questionTerminal/ffb643da9f094e80949d58f1db56a438?orderByHotValue=1&page=1&onlyReference=false)
+ [深度学习之RNN(循环神经网络)](https://blog.csdn.net/qq_32241189/article/details/80461635)
+ [TSNE——目前最好的降维方法](https://www.cnblogs.com/bonelee/p/7849867.html)
+ [判断一个栈的出栈序列的合法性](https://blog.csdn.net/lishanleilixin/article/details/89433774)